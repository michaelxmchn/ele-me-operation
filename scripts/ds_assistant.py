#!/usr/bin/env python3
"""
DeepSeek æœ¬åœ°è¾…åŠ©ç³»ç»Ÿ
ä¸“é—¨å¤„ç†å¤æ‚é€»è¾‘ä¼˜åŒ–ï¼Œå‡å°‘ Token æ¶ˆè€—
"""

import json
import hashlib
import os
from datetime import datetime
from typing import Any, Dict, Optional
from functools import lru_cache

# é…ç½®
CACHE_DIR = "/home/michael/.openclaw/workspace/.ds_cache"
OPTIMIZATION_LOG = "/home/michael/.openclaw/workspace/.ds_optimizations.log"

class DeepSeekAssistant:
    """æœ¬åœ° DeepSeek è¾…åŠ©ç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or "sk-f04a00d9f3d54cc2861552fd46e8ed76"
        self.api_url = "https://api.deepseek.com/chat/completions"
        self.model = "deepseek-chat"
        self.cache_dir = CACHE_DIR
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def _hash_data(self, data: Any) -> str:
        """ç”Ÿæˆæ•°æ®å“ˆå¸Œ"""
        return hashlib.md5(str(data).encode()).hexdigest()[:16]
    
    def _get_cache_path(self, data_hash: str) -> str:
        """è·å–ç¼“å­˜è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{data_hash}.json")
    
    def _load_cache(self, data_hash: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        path = self._get_cache_path(data_hash)
        if os.path.exists(path):
            with open(path, "r") as f:
                return json.load(f)
        return None
    
    def _save_cache(self, data_hash: str, result: Dict):
        """ä¿å­˜ç¼“å­˜"""
        path = self._get_cache_path(data_hash)
        with open(path, "w") as f:
            json.dump(result, f, indent=2)
    
    def _log_optimization(self, task_type: str, original_tokens: int, optimized_tokens: int):
        """è®°å½•ä¼˜åŒ–æ•ˆæœ"""
        log = {
            "time": datetime.now().isoformat(),
            "task_type": task_type,
            "original_tokens": original_tokens,
            "optimized_tokens": optimized_tokens,
            "savings": f"{(1 - optimized_tokens/original_tokens)*100:.0f}%"
        }
        
        with open(OPTIMIZATION_LOG, "a") as f:
            f.write(json.dumps(log) + "\n")
    
    @lru_cache(maxsize=50)
    def optimize_script(self, script_content: str, focus: str = "token") -> Dict:
        """
        ä¼˜åŒ–ä»£ç è„šæœ¬
        
        Args:
            script_content: è„šæœ¬å†…å®¹
            focus: ä¼˜åŒ–é‡ç‚¹ (token/performance/both)
        
        Returns:
            ä¼˜åŒ–å»ºè®®
        """
        task_type = f"script_optimize_{focus}"
        data_hash = self._hash_data(script_content + focus)
        
        # æ£€æŸ¥ç¼“å­˜
        cached = self._load_cache(data_hash)
        if cached:
            print(f"âœ… ä½¿ç”¨ç¼“å­˜: {task_type}")
            return cached
        
        # æ„å»ºä¼˜åŒ–æç¤º
        prompt = f"""è¯·ä¼˜åŒ–ä»¥ä¸‹ Python ä»£ç ï¼Œ{focus}ç›¸å…³ä¼˜åŒ–ï¼š

{self._truncate(script_content, 3000)}

è¯·ç”¨ JSON æ ¼å¼è¿”å›ï¼š
{{
    "summary": "ä¼˜åŒ–æ¦‚è¿°",
    "changes": ["æ”¹åŠ¨1", "æ”¹åŠ¨2"],
    "estimated_savings": "é¢„ä¼°æ”¶ç›Š",
    "code_suggestions": [{{"before": "åŸä»£ç ", "after": "ä¼˜åŒ–å", "reason": "åŸå› "}}]
}}
"""
        
        result = self._call_deepseek(prompt, max_tokens=1500)
        
        if result:
            self._save_cache(data_hash, result)
            self._log_optimization(task_type, 3000, len(prompt))
        
        return result
    
    def analyze_logic(self, problem: str, context: str = "") -> Dict:
        """
        åˆ†æå¤æ‚é€»è¾‘é—®é¢˜
        
        Args:
            problem: é—®é¢˜æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            åˆ†æç»“æœ
        """
        task_type = "logic_analysis"
        data_hash = self._hash_data(problem + context)
        
        cached = self._load_cache(data_hash)
        if cached:
            return cached
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é€»è¾‘é—®é¢˜ï¼š

é—®é¢˜ï¼š{problem}

ä¸Šä¸‹æ–‡ï¼š{context}

è¯·ç”¨ JSON æ ¼å¼è¿”å›ï¼š
{{
    "analysis": "é—®é¢˜åˆ†æ",
    "approaches": ["æ–¹æ¡ˆ1", "æ–¹æ¡ˆ2"],
    "recommendation": "æ¨èæ–¹æ¡ˆ",
    "implementation_notes": ["æ³¨æ„1", "æ³¨æ„2"]
}}
"""
        
        result = self._call_deepseek(prompt, max_tokens=800)
        
        if result:
            self._save_cache(data_hash, result)
        
        return result
    
    def generate_code(self, requirement: str, language: str = "python") -> Dict:
        """
        ç”Ÿæˆä»£ç 
        
        Args:
            requirement: åŠŸèƒ½éœ€æ±‚
            language: ç¼–ç¨‹è¯­è¨€
        
        Returns:
            ç”Ÿæˆçš„ä»£ç 
        """
        task_type = f"code_generation_{language}"
        data_hash = self._hash_data(requirement)
        
        cached = self._load_cache(data_hash)
        if cached:
            return cached
        
        prompt = f"""è¯·ç”¨ {language} å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š

{requirement}

è¯·ç”¨ JSON æ ¼å¼è¿”å›ï¼š
{{
    "code": "ä»£ç ",
    "explanation": "è¯´æ˜",
    "usage_example": "ä½¿ç”¨ç¤ºä¾‹"
}}
"""
        
        result = self._call_deepseek(prompt, max_tokens=1200)
        
        if result:
            self._save_cache(data_hash, result)
        
        return result
    
    def optimize_prompt(self, original_prompt: str, goal: str = "reduce_tokens") -> Dict:
        """
        ä¼˜åŒ–æç¤ºè¯
        
        Args:
            original_prompt: åŸå§‹æç¤ºè¯
            goal: ä¼˜åŒ–ç›®æ ‡ (reduce_tokens/improve_accuracy/both)
        
        Returns:
            ä¼˜åŒ–åçš„æç¤ºè¯
        """
        task_type = f"prompt_optimize_{goal}"
        data_hash = self._hash_data(original_prompt)
        
        cached = self._load_cache(data_hash)
        if cached:
            return cached
        
        prompt = f"""è¯·ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ï¼Œç›®æ ‡ï¼š{goal}

åŸå§‹æç¤ºè¯ï¼š
{self._truncate(original_prompt, 2000)}

è¯·ç”¨ JSON æ ¼å¼è¿”å›ï¼š
{{
    "optimized_prompt": "ä¼˜åŒ–åçš„æç¤ºè¯",
    "changes": ["æ”¹åŠ¨1", "æ”¹åŠ¨2"],
    "estimated_token_reduction": "é¢„ä¼°tokenå‡å°‘ç™¾åˆ†æ¯”",
    "quality_impact": "å¯¹è¾“å‡ºè´¨é‡çš„å½±å“"
}}
"""
        
        result = self._call_deepseek(prompt, max_tokens=1000)
        
        if result:
            self._save_cache(data_hash, result)
            original_len = len(original_prompt)
            optimized_len = len(result.get("optimized_prompt", ""))
            self._log_optimization(task_type, original_len, optimized_len)
        
        return result
    
    def _call_deepseek(self, prompt: str, max_tokens: int = 1000) -> Optional[Dict]:
        """è°ƒç”¨ DeepSeek API"""
        try:
            import requests
            
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": 0.5,
                    "top_p": 0.9
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                start, end = content.find("{"), content.rfind("}") + 1
                if start != -1 and end != 0:
                    return json.loads(content[start:end])
            
            print(f"âŒ APIé”™è¯¯: {response.status_code}")
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        
        return None
    
    def _truncate(self, text: str, max_len: int) -> str:
        """æˆªæ–­æ–‡æœ¬"""
        if len(text) <= max_len:
            return text
        return text[:max_len] + "...[æˆªæ–­]"
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        files = os.listdir(self.cache_dir) if os.path.exists(self.cache_dir) else []
        
        return {
            "cache_count": len(files),
            "cache_dir": self.cache_dir
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        import shutil
        if os.path.exists(self.cache_dir):
            shutil.rmtree(self.cache_dir)
            os.makedirs(self.cache_dir)
            print("âœ… ç¼“å­˜å·²æ¸…ç©º")


# ä¾¿æ·å‡½æ•°
def quick_optimize(script: str) -> Dict:
    """å¿«é€Ÿä¼˜åŒ–è„šæœ¬ï¼ˆé»˜è®¤ token ä¼˜åŒ–ï¼‰"""
    assistant = DeepSeekAssistant()
    return assistant.optimize_script(script, "token")


def analyze_problem(problem: str) -> Dict:
    """åˆ†æé€»è¾‘é—®é¢˜"""
    assistant = DeepSeekAssistant()
    return assistant.analyze_logic(problem)


def optimize_prompt(prompt: str) -> Dict:
    """ä¼˜åŒ–æç¤ºè¯"""
    assistant = DeepSeekAssistant()
    return assistant.optimize_prompt(prompt, "reduce_tokens")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§  DeepSeek æœ¬åœ°è¾…åŠ©ç³»ç»Ÿ")
    print("=" * 60)
    
    assistant = DeepSeekAssistant()
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = assistant.get_cache_stats()
    print(f"\nğŸ“¦ ç¼“å­˜ç»Ÿè®¡: {stats['cache_count']} ä¸ªç¼“å­˜é¡¹")
    
    print("\nå¯ç”¨åŠŸèƒ½:")
    print("  â€¢ optimize_script(script, focus) - ä¼˜åŒ–ä»£ç ")
    print("  â€¢ analyze_logic(problem, context) - åˆ†æé€»è¾‘")
    print("  â€¢ generate_code(requirement) - ç”Ÿæˆä»£ç ")
    print("  â€¢ optimize_prompt(prompt) - ä¼˜åŒ–æç¤ºè¯")
    
    print("\n" + "=" * 60)
