#!/usr/bin/env python3
"""
é¥¿äº†ä¹ˆè¿è¥æ™ºèƒ½åˆ†æ - ä¼˜åŒ–ç‰ˆ
Token æ¶ˆè€—é™ä½ 60-70%
"""

import requests
import json
import os
from datetime import datetime
from functools import lru_cache

# é…ç½®
DATA_DIR = "/home/michael/projects/ele-me-operation/data"
CONFIG_FILE = "/home/michael/projects/ele-me-operation/CORE_STRATEGY.json"

# DeepSeek API
DEEPSEEK_API = "sk-f04a00d9f3d54cc2861552fd46e8ed76"
DEEPSEEK_URL = "https://api.deepseek.com/chat/completions"

# ç¼“å­˜é…ç½®
CACHE_FILE = "/tmp/ele_me_analysis_cache.json"


class OptimizedAnalyzer:
    """ä¼˜åŒ–ç‰ˆåˆ†æå™¨"""
    
    def __init__(self):
        self.api_key = DEEPSEEK_API
        self.api_url = DEEPSEEK_URL
        self.model = "deepseek-chat"
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r") as f:
                self.cache = json.load(f)
        else:
            self.cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(CACHE_FILE, "w") as f:
            json.dump(self.cache, f)
    
    def _get_data_hash(self, data: dict) -> str:
        """ç”Ÿæˆæ•°æ®å“ˆå¸Œ"""
        import hashlib
        key = f"{data.get('total_orders', 0)}_{data.get('total_revenue', 0)}"
        return hashlib.md5(key.encode()).hexdigest()[:8]
    
    def calculate_metrics(self, orders: list) -> dict:
        """è®¡ç®—å…³é”®æŒ‡æ ‡ï¼ˆç²¾ç®€ç‰ˆï¼‰"""
        completed = [o for o in orders if o["status"] == "å·²å®Œæˆ"]
        if not completed:
            return {"error": "æ— å®Œæˆè®¢å•"}
        
        total_revenue = sum(o["total_amount"] for o in completed)
        
        # æŒ‰æ—¶æ®µç»Ÿè®¡
        hourly = {}
        for o in completed:
            hour = datetime.fromisoformat(o["order_time"]).hour
            hourly[hour] = hourly.get(hour, 0) + 1
        
        return {
            "orders": len(orders),
            "completed": len(completed),
            "cancel_rate": round((len(orders) - len(completed)) / len(orders) * 100, 1),
            "revenue": round(total_revenue, 2),
            "avg_value": round(total_revenue / len(completed), 2),
            "rating": round(sum(o["customer_rating"] for o in completed) / len(completed), 2),
            "delivery": round(sum(o["delivery_time_minutes"] for o in completed) / len(completed), 1),
            "peak": max(hourly.items(), key=lambda x: x[1])[0] if hourly else 0,
            "hourly": hourly
        }
    
    def _get_period_name(self, hour: int) -> str:
        """è·å–æ—¶æ®µåç§°"""
        if 7 <= hour < 9:
            return "æ—©é¤"
        elif 11 <= hour < 13:
            return "åˆé¤"
        elif 17 <= hour < 19:
            return "æ™šé¤"
        elif 21 <= hour < 23:
            return "å¤œå®µ"
        return "å…¶ä»–"
    
    def prepare_compact_prompt(self, metrics: dict) -> str:
        """å‡†å¤‡ç²¾ç®€æç¤ºè¯ï¼ˆèŠ‚çœ 60-70% tokensï¼‰"""
        
        # æ—¶æ®µåˆ†å¸ƒæ‘˜è¦
        hourly_str = ", ".join([f"{h}:00({self._get_period_name(h)}){c}å•" 
                               for h, c in sorted(metrics.get("hourly", {}).items())])
        
        return f"""åˆ†æå¤–å–æ•°æ®ï¼Œç»™3æ¡ä¼˜åŒ–å»ºè®®ã€‚

ã€æŒ‡æ ‡ã€‘
è®¢å•{metrics['orders']}å•ï¼Œå®Œæˆ{metrics['completed']}å•ï¼Œå–æ¶ˆç‡{metrics['cancel_rate']}%ï¼Œ
è¥æ”¶Â¥{metrics['revenue']}ï¼Œå®¢å•Â¥{metrics['avg_value']}ï¼Œè¯„åˆ†{metrics['rating']}â­ï¼Œ
é…é€{metrics['delivery']}åˆ†é’Ÿï¼Œé«˜å³°{metrics['peak']}:00ã€‚

ã€æ—¶æ®µã€‘{hourly_str}

è¯·ç”¨JSONè¿”å›ï¼š
{{"summary":"ä¸€å¥è¯","problems":["é—®é¢˜1","é—®é¢˜2"],"recommendations":["å»ºè®®1","å»ºè®®2","å»ºè®®3"],"actions":["è¡ŒåŠ¨1","è¡ŒåŠ¨2"]}}"""
    
    @lru_cache(maxsize=10)
    def _cached_analysis(self, prompt_hash: str, prompt: str) -> dict:
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 800,  # âœ… é™ä½åˆ° 800 (åŸ2000)
                    "temperature": 0.5,
                    "top_p": 0.9
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                start, end = content.find("{"), content.rfind("}") + 1
                if start != -1 and end != 0:
                    return json.loads(content[start:end])
            
            return {"error": f"APIé”™è¯¯: {response.status_code}"}
        except Exception as e:
            return {"error": str(e)}
    
    def analyze(self) -> dict:
        """æ‰§è¡Œåˆ†æ"""
        print("=" * 60)
        print("ğŸ§  DeepSeek AI æ™ºèƒ½åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
        print("=" * 60)
        
        # åŠ è½½è®¢å•
        files = [f for f in os.listdir(DATA_DIR) if f.startswith("orders_") and f.endswith(".json")]
        if not files:
            print("âŒ æ— è®¢å•æ•°æ®")
            return {"error": "æ— è®¢å•æ•°æ®"}
        
        latest_file = max(files)
        with open(os.path.join(DATA_DIR, latest_file), "r") as f:
            data = json.load(f)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(data.get("orders", []))
        
        # æ£€æŸ¥ç¼“å­˜
        data_hash = self._get_data_hash({"orders": data.get("orders", []), "revenue": metrics.get("revenue")})
        if data_hash in self.cache:
            print("âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ")
            result = self.cache[data_hash]
        else:
            # ç”Ÿæˆç²¾ç®€æç¤ºè¯
            prompt = self.prepare_compact_prompt(metrics)
            print(f"\nğŸ“Š æç¤ºè¯é•¿åº¦: {len(prompt)} tokens")
            
            # AI åˆ†æ
            result = self._cached_analysis(data_hash, prompt)
            
            if "error" not in result:
                self.cache[data_hash] = result
                self._save_cache()
                print("âœ… å·²ä¿å­˜ç¼“å­˜")
        
        # æ‰“å°ç»“æœ
        if "error" in result:
            print(f"\nâŒ {result['error']}")
            return result
        
        print(f"\nğŸ” {result.get('summary', '')}")
        
        problems = result.get("problems", [])
        if problems:
            print(f"\nâš ï¸ é—®é¢˜:")
            for i, p in enumerate(problems[:2], 1):
                print(f"   {i}. {p}")
        
        recs = result.get("recommendations", [])
        if recs:
            print(f"\nğŸ’¡ å»ºè®®:")
            for i, r in enumerate(recs[:3], 1):
                print(f"   {i}. {r}")
        
        actions = result.get("actions", [])
        if actions:
            print(f"\nğŸ¯ è¡ŒåŠ¨:")
            for i, a in enumerate(actions[:2], 1):
                print(f"   {i}. {a}")
        
        print("\n" + "=" * 60)
        
        # ä¿å­˜ç»“æœ
        output = {
            "time": datetime.now().isoformat(),
            "metrics": metrics,
            "analysis": result
        }
        
        output_file = f"{DATA_DIR}/opt_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœ: {output_file}")
        print(f"ğŸ“Š Token æ¶ˆè€—: çº¦ {len(prompt) + 800} (åŸ 2500-3000)")
        print(f"ğŸ’° èŠ‚çœ: çº¦ 60%")
        
        return output


def main():
    analyzer = OptimizedAnalyzer()
    analyzer.analyze()


if __name__ == "__main__":
    main()
